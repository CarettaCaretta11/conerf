{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca221cf-1065-49ca-812f-99dddddff0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorboard\n",
    "tensorboard.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b6bc7f-e71b-454c-b7c2-e16c2f4315ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from flax.metrics import tensorboard as flax_tb\n",
    "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1194f6b0-86b7-4141-b575-3d32423e69b4",
   "metadata": {},
   "source": [
    "## Related methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa545f4-6b32-4751-ab48-a6e89c304ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = Path(\"../experiments/main-experiments//\")\n",
    "summaries_dir = \"summaries/eval\"\n",
    "\n",
    "\n",
    "data = []\n",
    "valid_methods = [\n",
    "    \"hypernerf_ds\",\n",
    "    \"nerf_latent\",\n",
    "    \"nerf\",\n",
    "    \"nerfies\",\n",
    "    \n",
    "]\n",
    "\n",
    "for method in log_dir.glob(\"*\"):\n",
    "    method_name = method.name.split(\"-\")[0]\n",
    "    dataset_name = \"-\".join(method.name.split(\"-\")[1:])    \n",
    "    if method_name not in valid_methods:\n",
    "        continue\n",
    "        \n",
    "    event_accumulator = EventAccumulator(path=(method / summaries_dir).as_posix())\n",
    "    event_accumulator.Reload()\n",
    "    try:\n",
    "        events_psnr = event_accumulator.Tensors(\"metrics-eval/psnr/val\")\n",
    "        events_mssim = event_accumulator.Tensors(\"metrics-eval/ms_ssim/val\")\n",
    "        events_lpips = event_accumulator.Tensors(\"metrics-eval/lpips/val\")\n",
    "    except Exception as e:\n",
    "        print(method / summaries_dir)\n",
    "        print(e)\n",
    "    \n",
    "    psnr = [flax_tb.tf.make_ndarray(x.tensor_proto) for x in events_psnr][0]\n",
    "    ms_sim = [flax_tb.tf.make_ndarray(x.tensor_proto) for x in events_mssim][0]\n",
    "    lpips = [flax_tb.tf.make_ndarray(x.tensor_proto) for x in events_lpips][0]\n",
    "    data.append({\n",
    "        \"method\": method_name,\n",
    "        \"dataset\": dataset_name,\n",
    "        \"psnr\": psnr,\n",
    "        \"msssim\": ms_sim,\n",
    "        \"lpips\": lpips\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c923a96-0ec3-40a2-8ad9-d4c6cb655b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = Path(\"../experiments/proj-experiments/\")\n",
    "summaries_dir = \"summaries/eval\"\n",
    "\n",
    "valid_methods = [\n",
    "    \"hypernerf_ds_proj\",    \n",
    "]\n",
    "\n",
    "for method in log_dir.glob(\"*\"):\n",
    "    method_name = method.name.split(\"-\")[0]\n",
    "    dataset_name = \"-\".join(method.name.split(\"-\")[1:])    \n",
    "    if method_name not in valid_methods:\n",
    "        continue\n",
    "        \n",
    "    event_accumulator = EventAccumulator(path=(method / summaries_dir).as_posix())\n",
    "    event_accumulator.Reload()\n",
    "    try:\n",
    "        events_psnr = event_accumulator.Tensors(\"metrics-eval/psnr/val\")\n",
    "        events_mssim = event_accumulator.Tensors(\"metrics-eval/ms_ssim/val\")\n",
    "        events_lpips = event_accumulator.Tensors(\"metrics-eval/lpips/val\")\n",
    "    except Exception as e:\n",
    "        print(method / summaries_dir)\n",
    "        print(e)\n",
    "    \n",
    "    psnr = [flax_tb.tf.make_ndarray(x.tensor_proto) for x in events_psnr][0]\n",
    "    ms_sim = [flax_tb.tf.make_ndarray(x.tensor_proto) for x in events_mssim][0]\n",
    "    lpips = [flax_tb.tf.make_ndarray(x.tensor_proto) for x in events_lpips][0]\n",
    "    data.append({\n",
    "        \"method\": method_name,\n",
    "        \"dataset\": dataset_name,\n",
    "        \"psnr\": psnr,\n",
    "        \"msssim\": ms_sim,\n",
    "        \"lpips\": lpips\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fd5c31-8dc8-4a69-8076-c30129c5ee0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_results = pd.DataFrame(data).groupby(\"method\").mean()\n",
    "\n",
    "grouped_results = grouped_results.reindex(\n",
    "    [\n",
    "        'nerf', \n",
    "        'nerf_latent', \n",
    "        'nerfies',  \n",
    "        'hypernerf_ds',\n",
    "        'hypernerf_ds_proj',\n",
    "    ]\n",
    ").reset_index()\n",
    "grouped_results = grouped_results.replace({\n",
    "    \"hypernerf_ds\": \"HyperNeRF DS \\\\cite{park2021hypernerf}\",\n",
    "    \"nerf\": \"NeRF\",\n",
    "    \"nerf_latent\": \"NeRF + Latent \\\\cite{mildenhall2020nerf}\",\n",
    "    \"nerfies\": \"NeRFies \\\\cite{park2020deformable}\",\n",
    "    \"hypernerf_ds_proj\": \"Ours{-}$\\MaskNet$\",\n",
    "})\n",
    "print(grouped_results.to_latex(escape=False, float_format=\"%.3f\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d309811c-14f0-4ff7-bd75-3d59e0d2aa1d",
   "metadata": {},
   "source": [
    "## Kubric main table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ff78af-351b-4131-a63f-5caaefd771b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = Path(\"../experiments/kubric-proj-experiments-color/\")\n",
    "summaries_dir = \"summaries/eval\"\n",
    "\n",
    "\n",
    "data = []\n",
    "\n",
    "for method in log_dir.glob(\"*\"):\n",
    "    method_name = method.name.split(\"-\")[0]\n",
    "    dataset_name = \"-\".join(method.name.split(\"-\")[1:])\n",
    "    \n",
    "    event_accumulator = EventAccumulator(path=(method / summaries_dir).as_posix())\n",
    "    event_accumulator.Reload()\n",
    "    try:\n",
    "        events_psnr = event_accumulator.Tensors(\"metrics-eval/psnr/train\")\n",
    "        events_mssim = event_accumulator.Tensors(\"metrics-eval/ms_ssim/train\")\n",
    "        events_lpips = event_accumulator.Tensors(\"metrics-eval/lpips/train\")\n",
    "    except Exception as e:\n",
    "        print(method / summaries_dir)\n",
    "        print(e)\n",
    "    \n",
    "    psnr = [flax_tb.tf.make_ndarray(x.tensor_proto) for x in events_psnr][0]\n",
    "    ms_sim = [flax_tb.tf.make_ndarray(x.tensor_proto) for x in events_mssim][0]\n",
    "    lpips = [flax_tb.tf.make_ndarray(x.tensor_proto) for x in events_lpips][0]\n",
    "    data.append({\n",
    "        \"method\": method_name,\n",
    "        \"dataset\": dataset_name,\n",
    "        \"psnr\": psnr,\n",
    "        \"msssim\": ms_sim,\n",
    "        \"lpips\": lpips\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c768323d-f501-46b6-a000-89da68cfbc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = Path(\"../experiments/ablations-final-color/\")\n",
    "summaries_dir = \"summaries/eval\"\n",
    "\n",
    "for method in log_dir.glob(\"*\"):\n",
    "    method_name = method.name.split(\"-\")[0]\n",
    "    dataset_name = \"-\".join(method.name.split(\"-\")[1:-1]) \n",
    "    ablation_type = method.name.split(\"-\")[-1]\n",
    "    \n",
    "    if dataset_name != \"processed-trio-train-color-fixed\" or ablation_type != \"enc_attr_mask\":\n",
    "        continue\n",
    "        \n",
    "    event_accumulator = EventAccumulator(path=(method / summaries_dir).as_posix())\n",
    "    event_accumulator.Reload()\n",
    "    try:\n",
    "        events_psnr = event_accumulator.Tensors(\"metrics-eval/psnr/train\")\n",
    "        events_mssim = event_accumulator.Tensors(\"metrics-eval/ms_ssim/train\")\n",
    "        events_lpips = event_accumulator.Tensors(\"metrics-eval/lpips/train\")\n",
    "    except Exception as e:\n",
    "        print(method / summaries_dir)\n",
    "        print(e)\n",
    "    \n",
    "    psnr = [flax_tb.tf.make_ndarray(x.tensor_proto) for x in events_psnr][0]\n",
    "    ms_sim = [flax_tb.tf.make_ndarray(x.tensor_proto) for x in events_mssim][0]\n",
    "    lpips = [flax_tb.tf.make_ndarray(x.tensor_proto) for x in events_lpips][0]\n",
    "    data.append({\n",
    "        \"method\": method_name,\n",
    "        \"dataset\": dataset_name,\n",
    "        \"psnr\": psnr,\n",
    "        \"msssim\": ms_sim,\n",
    "        \"lpips\": lpips\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5ac3fe-54ae-4247-b51f-4603141dcaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e2caba-b3af-4c45-9476-68cccf7cea2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_results = pd.DataFrame(data).groupby(\"method\").mean().reset_index()\n",
    "grouped_results = grouped_results.replace({\n",
    "    \"hypernerf_ds\": \"HyperNeRF \\\\cite{park2021hypernerf}\",\n",
    "    \"hypernerf_ds_proj\": \"Ours{-}$\\MaskNet$\",\n",
    "    \"ours\": \"Ours\"\n",
    "})\n",
    "\n",
    "print(grouped_results.to_latex(escape=False, float_format=\"%.3f\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f24002-9c7f-4a6f-bb87-350406638f30",
   "metadata": {},
   "source": [
    "## Kubric ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639c705f-5a12-464e-a529-1f9f9f200b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = Path(\"../experiments/ablations-final-color/\")\n",
    "summaries_dir = \"summaries/eval\"\n",
    "data = []\n",
    "\n",
    "for method in log_dir.glob(\"*\"):\n",
    "    method_name = method.name.split(\"-\")[0]\n",
    "    dataset_name = \"-\".join(method.name.split(\"-\")[1:-1]) \n",
    "    ablation_type = method.name.split(\"-\")[-1]\n",
    "    print(method)\n",
    "    if dataset_name != \"processed-trio-train-color-fixed\" or \"lap\" in ablation_type:\n",
    "        continue\n",
    "        \n",
    "    event_accumulator = EventAccumulator(path=(method / summaries_dir).as_posix())\n",
    "    event_accumulator.Reload()\n",
    "    try:\n",
    "        events_psnr = event_accumulator.Tensors(\"metrics-eval/psnr/train\")\n",
    "        events_mssim = event_accumulator.Tensors(\"metrics-eval/ms_ssim/train\")\n",
    "        events_lpips = event_accumulator.Tensors(\"metrics-eval/lpips/train\")\n",
    "    except Exception as e:\n",
    "        print(method / summaries_dir)\n",
    "        print(e)\n",
    "    \n",
    "    psnr = [flax_tb.tf.make_ndarray(x.tensor_proto) for x in events_psnr][0]\n",
    "    ms_sim = [flax_tb.tf.make_ndarray(x.tensor_proto) for x in events_mssim][0]\n",
    "    lpips = [flax_tb.tf.make_ndarray(x.tensor_proto) for x in events_lpips][0]\n",
    "    data.append({\n",
    "        \"dataset\": dataset_name,\n",
    "        \"ablation\": ablation_type,\n",
    "        \"psnr\": psnr,\n",
    "        \"msssim\": ms_sim,\n",
    "        \"lpips\": lpips\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91713e88-2717-4b5c-96ac-10e6577f9d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_results = pd.DataFrame(data).groupby(\"ablation\").mean()\n",
    "grouped_results = grouped_results.reindex([\"base\", \"enc\", \"enc_attr\", \"enc_attr_mask\"]).reset_index()\n",
    "grouped_results = grouped_results.replace({\n",
    "    \"base\": \"$\\loss{recon}$\",\n",
    "    \"enc\": \"$\\loss{recon} +\\loss{enc}$\",\n",
    "    \"enc_attr\": \"$\\loss{recon} + \\loss{enc} +\\loss{attr}$\",\n",
    "    \"enc_attr_mask\": \"$\\loss{recon} + \\loss{enc} + \\loss{attr} +\\loss{mask}$\",\n",
    "})\n",
    "with pd.option_context(\"max_colwidth\", 1000):\n",
    "    print(grouped_results.to_latex(escape=False, float_format=\"%.3f\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf7e2f1-e1b5-4726-9e50-eef20e5189f5",
   "metadata": {},
   "source": [
    "## Main ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e131b5-9e33-4637-aff0-d6d592095537",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = Path(\"../experiments/ablations/\")\n",
    "summaries_dir = \"summaries/eval\"\n",
    "\n",
    "data = []\n",
    "\n",
    "for method in log_dir.glob(\"*\"):\n",
    "    method_name = method.name.split(\"-\")[0]\n",
    "    dataset_name = \"-\".join(method.name.split(\"-\")[1:-1])    \n",
    "    ablation_type = method.name.split(\"-\")[-1]\n",
    "    \n",
    "    if \"proj\" in method_name or dataset_name == \"processed-trio-train\" or \"lap\" in ablation_type:\n",
    "        continue\n",
    "        \n",
    "    event_accumulator = EventAccumulator(path=(method / summaries_dir).as_posix())\n",
    "    event_accumulator.Reload()\n",
    "    try:\n",
    "        events_psnr = event_accumulator.Tensors(\"metrics-eval/psnr/val\")\n",
    "        events_mssim = event_accumulator.Tensors(\"metrics-eval/ms_ssim/val\")\n",
    "        events_lpips = event_accumulator.Tensors(\"metrics-eval/lpips/val\")\n",
    "    except Exception as e:\n",
    "        print(method / summaries_dir)\n",
    "        print(e)\n",
    "    \n",
    "    psnr = [flax_tb.tf.make_ndarray(x.tensor_proto) for x in events_psnr][0]\n",
    "    ms_sim = [flax_tb.tf.make_ndarray(x.tensor_proto) for x in events_mssim][0]\n",
    "    lpips = [flax_tb.tf.make_ndarray(x.tensor_proto) for x in events_lpips][0]\n",
    "    data.append({\n",
    "        \"dataset\": dataset_name,\n",
    "        \"ablation\": ablation_type,\n",
    "        \"psnr\": psnr,\n",
    "        \"msssim\": ms_sim,\n",
    "        \"lpips\": lpips\n",
    "    })\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cc719e-a685-4574-b97d-725137b6f993",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_results = pd.DataFrame(data).groupby(\"ablation\").mean()\n",
    "grouped_results = grouped_results.reindex([\"base\", \"enc\", \"enc_attr\", \"enc_attr_mask\"]).reset_index()\n",
    "grouped_results = grouped_results.replace({\n",
    "    \"base\": \"$\\loss{recon}$\",\n",
    "    \"enc\": \"$\\loss{recon} +\\loss{enc}$\",\n",
    "    \"enc_attr\": \"$\\loss{recon} + \\loss{enc} +\\loss{attr}$\",\n",
    "    \"enc_attr_mask\": \"$\\loss{recon} + \\loss{enc} + \\loss{attr} +\\loss{mask}$\",\n",
    "})\n",
    "\n",
    "with pd.option_context(\"max_colwidth\", 1000):\n",
    "    print(grouped_results.to_latex(escape=False, float_format=\"%.3f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7f7db4-4fc8-41d0-860e-4a0149ae74e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
